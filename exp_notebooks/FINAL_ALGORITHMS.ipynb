{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech-to-Text API Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import speech_v1p1beta1 as speech\n",
    "from google.cloud import storage\n",
    "\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Reshape, Bidirectional\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "import string\n",
    "import json \n",
    "import re\n",
    "\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/ekin/Downloads/hack.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Client Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to test credentials\n",
    "'''\n",
    "def implicit():\n",
    "    from google.cloud import storage\n",
    "\n",
    "    # If you don't specify credentials when constructing the client, the\n",
    "    # client library will look for credentials in the environment.\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Make an authenticated API request\n",
    "    buckets = list(storage_client.list_buckets())\n",
    "    print(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "implicit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[alternatives {\n",
      "  transcript: \"how old is the Brooklyn Bridge\"\n",
      "  confidence: 0.9823954\n",
      "}\n",
      "language_code: \"en-us\"\n",
      "]\n",
      "Transcript: how old is the Brooklyn Bridge\n",
      "Confidence: 0.9823954105377197\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test speech client and transcription API on a sample audio file. \n",
    "'''\n",
    "# Instantiates a client\n",
    "client = speech.SpeechClient()\n",
    "\n",
    "# The name of the audio file to transcribe\n",
    "gcs_uri = \"gs://cloud-samples-data/speech/brooklyn_bridge.raw\"\n",
    "\n",
    "audio = speech.RecognitionAudio(uri=gcs_uri)\n",
    "\n",
    "config = speech.RecognitionConfig(\n",
    "    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "    sample_rate_hertz=16000,\n",
    "    language_code=\"en-US\",\n",
    ")\n",
    "\n",
    "# Detects speech in the audio file\n",
    "response = client.recognize(config=config, audio=audio)\n",
    "\n",
    "print(response.results)\n",
    "\n",
    "for result in response.results:\n",
    "    print(\"Transcript: {}\".format(result.alternatives[0].transcript))\n",
    "    print(\"Confidence: {}\".format(result.alternatives[0].confidence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing Long Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_gcs(gcs_uri):\n",
    "    \"\"\"Asynchronously transcribes the audio file specified by the gcs_uri.\"\"\"\n",
    "    client = speech.SpeechClient()\n",
    "\n",
    "    audio = speech.RecognitionAudio(uri=gcs_uri)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.FLAC,\n",
    "        sample_rate_hertz=48000,\n",
    "        audio_channel_count=2,\n",
    "        language_code=\"en-US\",\n",
    "        enable_automatic_punctuation=True,\n",
    "        enable_word_time_offsets=True, \n",
    "        enable_word_confidence=True,\n",
    "    )\n",
    "\n",
    "    operation = client.long_running_recognize(config=config, audio=audio)\n",
    "\n",
    "    print(\"Waiting for operation to complete...\")\n",
    "    response = operation.result(timeout=90)\n",
    "\n",
    "    # Each result is for a consecutive portion of the audio. Iterate through\n",
    "    # them to get the transcripts for the entire audio file.\n",
    "    for result in response.results:\n",
    "        # The first alternative is the most likely one for this portion.\n",
    "        alternative = result.alternatives[0]\n",
    "        print(u\"Transcript: {}\".format(result.alternatives[0].transcript))\n",
    "        print(\"Confidence: {}\".format(result.alternatives[0].confidence))\n",
    "        \n",
    "    return response.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for operation to complete...\n",
      "Transcript: Hi, my name is I can see you and today I'm going to be presenting our hack the Northeast project, which is I present. So first our presentation consists of three main components with front-end UI and ux developer has developed a web service which allows the user to\n",
      "Confidence: 0.9569842219352722\n"
     ]
    }
   ],
   "source": [
    "example_gcs_uri = \"gs://hack_the_ne/example_1.flac\"\n",
    "results = transcribe_gcs(example_gcs_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcript Processing Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = results[0].alternatives[0]\n",
    "transcript = result.transcript\n",
    "words = result.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, my name is I can see you and today I'm going to be presenting our hack the Northeast project, which is I present. So first our presentation consists of three main components with front-end UI and ux developer has developed a web service which allows the user to\n"
     ]
    }
   ],
   "source": [
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Remove punctuation from a `transcript` string\n",
    "'''\n",
    "def clean(transcript): \n",
    "    cleaned = transcript.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Temporarily remove `Laughter` from the transcript, but could be used later\n",
    "    cleaned = cleaned.replace('Laughter', '')\n",
    "    cleaned = cleaned.replace('Applause', '')\n",
    "    \n",
    "    # Remove hyphens \n",
    "    cleaned = cleaned.replace('â€”', '')\n",
    "    \n",
    "    # turn all characters into lowercase\n",
    "    return cleaned.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clarity\n",
    "Measurement of how understandable your speech is. Based on the confidence of the overall transcription. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clarity = result.confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brevity\n",
    "Count the number of filler_words or hedging language phrases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function returns a count of how many times \n",
    "each phrase in the `phrases` list is in the \n",
    "`transcript` string. \n",
    "'''\n",
    "def count_phrases(transcript, phrases, return_phrase_counts=True): \n",
    "    transcript = clean(transcript) # remove punctuation and lowercase\n",
    "    space_transcript = ' ' + transcript + ' '\n",
    "    phrase_counts = {}\n",
    "    all_counts = 0 \n",
    "    for phrase in phrases: \n",
    "        space_phrase = ' ' + phrase + ' '\n",
    "        count = space_transcript.count(space_phrase)\n",
    "        if count > 0: \n",
    "            phrase_counts[phrase] = count\n",
    "            all_counts = all_counts + count\n",
    "    \n",
    "    if return_phrase_counts: \n",
    "        return phrase_counts, all_counts # Return the counts for each filler phrase\n",
    "    else:\n",
    "        return all_counts # Only return the total number of counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "filler_words = [\n",
    "    'like', \n",
    "    'i mean',\n",
    "    'you know', \n",
    "    'so', \n",
    "    'well', \n",
    "    'you see', \n",
    "]\n",
    "\n",
    "hedging_language = [\n",
    "    'kind of', \n",
    "    'i think', \n",
    "    'maybe',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'like': 1, 'so': 1}\n"
     ]
    }
   ],
   "source": [
    "test_string = \"so I think I will begin with this like kind of interesting thing maybe\"\n",
    "filler_phrase_counts, filler_all_counts = count_phrases(test_string, filler_words)\n",
    "print(filler_phrase_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kind of': 1, 'i think': 1, 'maybe': 1}\n"
     ]
    }
   ],
   "source": [
    "hedging_phrase_counts, hedging_all_counts = count_phrases(test_string, hedging_language)\n",
    "print(hedging_phrase_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cadence\n",
    "Counting the number of words spoken every second. \n",
    "\n",
    "* Average is 130 words per minute | 2.166 words per second (avg)\n",
    "* Takes us around 0.46 seconds to speak one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute the total duration of the speech in seconds, given\n",
    "the dict of words. \n",
    "'''\n",
    "def speech_time(words): \n",
    "    first_word = words[0]\n",
    "    last_word = words[-1]\n",
    "    \n",
    "    start_time = first_word.start_time.total_seconds()\n",
    "    end_time = last_word.end_time.total_seconds()\n",
    "    \n",
    "    return end_time - start_time\n",
    "\n",
    "'''\n",
    "Computes the number of words spoken in the speech per \n",
    "second, given `words` as a dict. \n",
    "'''\n",
    "def words_per_second(words, time): \n",
    "    return len(words) / time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time:  15.3\n",
      "Pace (words / second):  0.9150326797385621\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Perform actual computations\n",
    "'''\n",
    "total_time = speech_time(words)\n",
    "pace = words_per_second(words, total_time)\n",
    "\n",
    "print('Total Time: ', total_time)\n",
    "print('Pace (words / second): ', pace) # slower than average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute the number of seconds for each word in `words`\n",
    "'''\n",
    "\n",
    "def seconds_per_word(words, verbose=False):\n",
    "    word_times = {}\n",
    "    for word_info in words:\n",
    "        word = word_info.word\n",
    "        start_time = word_info.start_time.total_seconds()\n",
    "        end_time = word_info.end_time.total_seconds()\n",
    "        confidence = word_info.confidence\n",
    "\n",
    "        if verbose: \n",
    "            print(\n",
    "                f\"{word}, start_time: {start_time}, end_time: {end_time}, confidence: {confidence}\"\n",
    "            )\n",
    "\n",
    "        time_of_word = round(end_time - start_time, 3)\n",
    "        word_times[word] = time_of_word\n",
    "        \n",
    "    return word_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm, start_time: 1.3, end_time: 1.9, confidence: 0.9876290559768677\n",
      "going, start_time: 1.9, end_time: 2.1, confidence: 0.9876290559768677\n",
      "to, start_time: 2.1, end_time: 2.2, confidence: 0.9876290559768677\n",
      "test, start_time: 2.2, end_time: 2.5, confidence: 1.0\n",
      "maybe, start_time: 2.5, end_time: 4.7, confidence: 0.9239681959152222\n",
      "that's, start_time: 4.7, end_time: 5.1, confidence: 0.9249534010887146\n",
      "a, start_time: 5.1, end_time: 5.4, confidence: 0.9668338894844055\n",
      "bit, start_time: 5.4, end_time: 5.6, confidence: 0.9876290559768677\n",
      "different, start_time: 5.6, end_time: 6.2, confidence: 1.0\n",
      "interesting, start_time: 6.2, end_time: 13.7, confidence: 0.8979593515396118\n",
      "absolutely, start_time: 13.7, end_time: 14.9, confidence: 0.9249765872955322\n",
      "totally, start_time: 14.9, end_time: 15.2, confidence: 0.9876290559768677\n",
      "no, start_time: 15.2, end_time: 16.4, confidence: 0.8444064855575562\n",
      "way, start_time: 16.4, end_time: 16.6, confidence: 0.8684027194976807\n",
      "{\"I'm\": 0.6, 'going': 0.2, 'to': 0.1, 'test': 0.3, 'maybe': 2.2, \"that's\": 0.4, 'a': 0.3, 'bit': 0.2, 'different': 0.6, 'interesting': 7.5, 'absolutely': 1.2, 'totally': 0.3, 'no': 1.2, 'way': 0.2}\n"
     ]
    }
   ],
   "source": [
    "word_times = seconds_per_word(words, verbose=True)\n",
    "# word_times = {k: v for k, v in sorted(word_times.items(), key=lambda item: item[1])} # sorted\n",
    "print(word_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: plot each word on a histogram and also plot an average line \n",
    "# simpler, just count the number of words that are greater than average, and compare\n",
    "def classify_pauses(baseline_time): \n",
    "    normal_words = {}\n",
    "    delayed_words = {}\n",
    "    avg_time = 0.46\n",
    "    for word, time in word_times.items(): \n",
    "        if time > avg_time: \n",
    "            delayed_words[word] = time\n",
    "        else:\n",
    "            normal_words[word] = time\n",
    "    return normal_words, delayed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal words:  {'going': 0.2, 'to': 0.1, 'test': 0.3, \"that's\": 0.4, 'a': 0.3, 'bit': 0.2, 'totally': 0.3, 'way': 0.2}\n",
      "Delayed words:  {\"I'm\": 0.6, 'maybe': 2.2, 'different': 0.6, 'interesting': 7.5, 'absolutely': 1.2, 'no': 1.2}\n"
     ]
    }
   ],
   "source": [
    "avg_time = 0.5 # half a second per word on avg. \n",
    "normal_words, delayed_words = classify_pauses(avg_time)\n",
    "print('Normal words: ', normal_words)\n",
    "print('Delayed words: ', delayed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strong Vocabulary (Passion/Urgency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze syntax\n",
    "# noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "# verb_phrases = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "# adjectives = [token.lemma_ for token in doc if token.pos_ == \"ADJ\"]\n",
    "\n",
    "# print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "# print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "# print(\"Adjectives:\", [token.lemma_ for token in doc if token.pos_ == \"ADJ\"])\n",
    "\n",
    "def count_repetitions(doc):\n",
    "    \n",
    "    lower_transcript = transcript.lower()\n",
    "    verb_phrases = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "    adjectives = [token.lemma_ for token in doc if token.pos_ == \"ADJ\"]\n",
    "    \n",
    "    verb_phrases = np.array(verb_phrases)\n",
    "    adjectives = np.array(adjectives)\n",
    "    \n",
    "    unique_verbs, counts_verbs = np.unique(verb_phrases, return_counts=True)\n",
    "    verbs = dict(zip(unique_verbs, counts_verbs))\n",
    "    \n",
    "    unique_adj, counts_adj = np.unique(adjectives, return_counts=True)\n",
    "    adj = dict(zip(unique_adj, counts_adj))\n",
    "    \n",
    "    return verbs, adj\n",
    "\n",
    "'''\n",
    "This function calculates how often the presenter uses each verb or \n",
    "adjective in order to determine their word choice variet. \n",
    "'''\n",
    "def perform_variety_analysis(transcript):\n",
    "    import spacy \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    lower_transcript = transcript.lower()\n",
    "    doc = nlp(lower_transcript)\n",
    "    \n",
    "    verbs, adj = count_repetitions(doc)\n",
    "    print(verbs)\n",
    "    print(adj)\n",
    "    \n",
    "    return verbs, adj\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, my name is I can see you and today I'm going to be presenting our hack the Northeast project, which is I present. So first our presentation consists of three main components with front-end UI and ux developer has developed a web service which allows the user to develop\n"
     ]
    }
   ],
   "source": [
    "tt = transcript + ' develop'\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'allow': 1, 'can': 1, 'consist': 1, 'develop': 2, 'go': 1, 'present': 1, 'see': 1}\n",
      "{'front': 1, 'main': 1, 'northeast': 1, 'present': 1}\n"
     ]
    }
   ],
   "source": [
    "verbs, adj = perform_variety_analysis(tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language_v1\n",
    "\n",
    "def sample_analyze_sentiment(text_content):\n",
    "    \"\"\"\n",
    "    Analyzing Sentiment in a String\n",
    "    \n",
    "    Returns dict of scores for the entire document, and a list of\n",
    "    dicts representing scores for each sentence in the document. \n",
    "\n",
    "    Args:\n",
    "      text_content The text content to analyze\n",
    "    \"\"\"\n",
    "\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "\n",
    "    # Available types: PLAIN_TEXT, HTML\n",
    "    type_ = language_v1.Document.Type.PLAIN_TEXT\n",
    "\n",
    "    language = \"en\"\n",
    "    document = {\"content\": text_content, \"type_\": type_, \"language\": language}\n",
    "\n",
    "    # Available values: NONE, UTF8, UTF16, UTF32\n",
    "    encoding_type = language_v1.EncodingType.UTF8\n",
    "\n",
    "    response = client.analyze_sentiment(request = {'document': document, 'encoding_type': encoding_type})\n",
    "    \n",
    "    # Get overall sentiment of the input document\n",
    "    print(u\"Document sentiment score: {}\".format(response.document_sentiment.score))\n",
    "    print(\n",
    "        u\"Document sentiment magnitude: {}\".format(\n",
    "            response.document_sentiment.magnitude\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Save doc stats to variable\n",
    "    doc_stats = {\n",
    "        'sentiment' : response.document_sentiment.score, \n",
    "        'magnitude' : response.document_sentiment.magnitude,\n",
    "    }\n",
    "    \n",
    "\n",
    "    # Get sentiment for all sentences in the document\n",
    "    sentences_stats = []\n",
    "    for sentence in response.sentences:\n",
    "        sentence_stats = {\n",
    "            'sentence' : sentence.text.content,\n",
    "            'sentiment' : sentence.sentiment.score, \n",
    "            'magnitude' : sentence.sentiment.magnitude,\n",
    "        }\n",
    "        \n",
    "        sentences_stats.append(sentence_stats)\n",
    "        \n",
    "        print(u\"Sentence text: {}\".format(sentence.text.content))\n",
    "        print(u\"Sentence sentiment score: {}\".format(sentence.sentiment.score))\n",
    "        print(u\"Sentence sentiment magnitude: {}\".format(sentence.sentiment.magnitude))\n",
    "\n",
    "    # Get the language of the text, which will be the same as\n",
    "    # the language specified in the request or, if not specified,\n",
    "    # the automatically-detected language.\n",
    "    print(u\"Language of the text: {}\".format(response.language))\n",
    "    \n",
    "    return doc_stats, sentences_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document sentiment score: 0.20000000298023224\n",
      "Document sentiment magnitude: 0.5\n",
      "Sentence text: Hi, my name is I can see you and today I'm going to be presenting our hack the Northeast project, which is I present.\n",
      "Sentence sentiment score: 0.10000000149011612\n",
      "Sentence sentiment magnitude: 0.10000000149011612\n",
      "Sentence text: So first our presentation consists of three main components with front-end UI and ux developer has developed a web service which allows the user to\n",
      "Sentence sentiment score: 0.4000000059604645\n",
      "Sentence sentiment magnitude: 0.4000000059604645\n",
      "Language of the text: en\n"
     ]
    }
   ],
   "source": [
    "test = 'My name is Ekin. Nice to meet you.'\n",
    "doc_stats, sentences_stats = sample_analyze_sentiment(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc stats:  {'sentiment': 0.20000000298023224, 'magnitude': 0.5}\n",
      "\n",
      "\n",
      "Sentences stats:  [{'sentence': \"Hi, my name is I can see you and today I'm going to be presenting our hack the Northeast project, which is I present.\", 'sentiment': 0.10000000149011612, 'magnitude': 0.10000000149011612}, {'sentence': 'So first our presentation consists of three main components with front-end UI and ux developer has developed a web service which allows the user to', 'sentiment': 0.4000000059604645, 'magnitude': 0.4000000059604645}]\n"
     ]
    }
   ],
   "source": [
    "print('Doc stats: ', doc_stats)\n",
    "print('\\n')\n",
    "print('Sentences stats: ', sentences_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TED Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wv():\n",
    "    # this line of code needs to run asynchronously first\n",
    "    wv = api.load('word2vec-google-news-300')\n",
    "    return wv\n",
    "\n",
    "def create_embedding(transcript, wv):\n",
    "#     print(transcript)\n",
    "    X = []\n",
    "    found_words = []\n",
    "    transcript = clean(transcript) # remove any punctuation\n",
    "    \n",
    "    print(transcript)\n",
    "    words = transcript.split()\n",
    "    for word in words: \n",
    "        try:\n",
    "            found_words.append(wv[word])\n",
    "        except: \n",
    "            continue\n",
    "            \n",
    "    embedding = np.asarray(found_words)\n",
    "    mean = np.mean(embedding, axis=0)\n",
    "    mean = mean.tolist()\n",
    "\n",
    "    if type(mean) == list: \n",
    "        X.append(mean)\n",
    "\n",
    "    X = np.array(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_analysis(transcript):\n",
    "    wv = load_wv()\n",
    "    embedding = create_embedding(transcript, wv)\n",
    "    \n",
    "    model_filepath = './ted_analysis_model'\n",
    "    model = tf.keras.models.load_model(model_filepath)\n",
    "    \n",
    "    pred = model.predict(embedding)[0]\n",
    "    cols = ['Beautiful', 'Confusing', 'Courageous', 'Funny', 'Informative', 'Ingenious', 'Inspiring', 'Longwinded', 'Unconvincing', 'Fascinating', 'Jaw-dropping', 'Persuasive', 'OK', 'Obnoxious']\n",
    "    \n",
    "    ted_dict = {}\n",
    "    for val, col in zip(pred, cols):\n",
    "        ted_dict[col] = val\n",
    "    return ted_dict # can be converted to a JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi my name is i can see you and today im going to be presenting our hack the northeast project which is i present so first our presentation consists of three main components with frontend ui and ux developer has developed a web service which allows the user to\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029282EE5E58> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "ted_dict = perform_analysis(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Beautiful': 0.08486959, 'Confusing': 0.025597274, 'Courageous': 0.004894078, 'Funny': 0.042433977, 'Informative': 0.38298982, 'Ingenious': 0.98094106, 'Inspiring': 0.28280404, 'Longwinded': 0.013673663, 'Unconvincing': 0.05666706, 'Fascinating': 0.9619733, 'Jaw-dropping': 0.59125984, 'Persuasive': 0.00097215176, 'OK': 0.21875376, 'Obnoxious': 0.05292952}\n"
     ]
    }
   ],
   "source": [
    "print(ted_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TED Analysis Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = load_wv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = create_embedding(transcript, wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filepath = './ted_analysis_model'\n",
    "model = tf.keras.models.load_model(model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(embedding)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm going to test maybe that's a bit different interesting absolutely totally no way\n"
     ]
    }
   ],
   "source": [
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Beautiful', 'Confusing', 'Courageous', 'Funny', 'Informative', 'Ingenious', 'Inspiring', 'Longwinded', 'Unconvincing', 'Fascinating', 'Jaw-dropping', 'Persuasive', 'OK', 'Obnoxious']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Beautiful': 0.11447036, 'Confusing': 0.10280144, 'Courageous': 0.0026097298, 'Funny': 0.81440914, 'Informative': 0.11125913, 'Ingenious': 0.7225983, 'Inspiring': 0.111739606, 'Longwinded': 0.026772916, 'Unconvincing': 0.09550491, 'Fascinating': 0.8304244, 'Jaw-dropping': 0.35544878, 'Persuasive': 0.0057587028, 'OK': 0.45375717, 'Obnoxious': 0.10564661}\n"
     ]
    }
   ],
   "source": [
    "ted_dict = {}\n",
    "for val, col in zip(pred, cols):\n",
    "    ted_dict[col] = val\n",
    "print(ted_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation Summary\n",
    "* Uses the pre-trained BERT model to summarize your presentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fea011bc1ff4b34941fc6e868412ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1344997306.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9638d6f47d49fd8005348af6c8454e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are you?(Laughter)It's been great, hasn't it? And the result is that we are educating people out of their creative capacities. And I used to be one, so there.(Laughter)And I like university professors, but you know, we shouldn't hold them up as the high-water mark of all human achievement. It wasn't an available condition.(Laughter)People weren't aware they could have that.(Laughter)Anyway, she went to see this specialist.\n"
     ]
    }
   ],
   "source": [
    "ted = \"\"\"Good morning. How are you?(Laughter)It's been great, hasn't it? I've been blown away by the whole thing. In fact, I'm leaving.(Laughter)There have been three themes running through the conference which are relevant to what I want to talk about. One is the extraordinary evidence of human creativity in all of the presentations that we've had and in all of the people here. Just the variety of it and the range of it. The second is that it's put us in a place where we have no idea what's going to happen, in terms of the future. No idea how this may play out.I have an interest in education. Actually, what I find is everybody has an interest in education. Don't you? I find this very interesting. If you're at a dinner party, and you say you work in education â€” Actually, you're not often at dinner parties, frankly.(Laughter)If you work in education, you're not asked.(Laughter)And you're never asked back, curiously. That's strange to me. But if you are, and you say to somebody, you know, they say, \"What do you do?\" and you say you work in education, you can see the blood run from their face. They're like, \"Oh my God,\" you know, \"Why me?\"(Laughter)\"My one night out all week.\"(Laughter)But if you ask about their education, they pin you to the wall. Because it's one of those things that goes deep with people, am I right? Like religion, and money and other things. So I have a big interest in education, and I think we all do. We have a huge vested interest in it, partly because it's education that's meant to take us into this future that we can't grasp. If you think of it, children starting school this year will be retiring in 2065. Nobody has a clue, despite all the expertise that's been on parade for the past four days, what the world will look like in five years' time. And yet we're meant to be educating them for it. So the unpredictability, I think, is extraordinary.And the third part of this is that we've all agreed, nonetheless, on the really extraordinary capacities that children have â€” their capacities for innovation. I mean, Sirena last night was a marvel, wasn't she? Just seeing what she could do. And she's exceptional, but I think she's not, so to speak, exceptional in the whole of childhood. What you have there is a person of extraordinary dedication who found a talent. And my contention is, all kids have tremendous talents. And we squander them, pretty ruthlessly.So I want to talk about education and I want to talk about creativity. My contention is that creativity now is as important in education as literacy, and we should treat it with the same status.(Applause) Thank you.(Applause)That was it, by the way. Thank you very much.(Laughter)So, 15 minutes left.(Laughter)Well, I was born... no.(Laughter)I heard a great story recently â€” I love telling it â€” of a little girl who was in a drawing lesson. She was six, and she was at the back, drawing, and the teacher said this girl hardly ever paid attention, and in this drawing lesson, she did. The teacher was fascinated. She went over to her, and she said, \"What are you drawing?\" And the girl said, \"I'm drawing a picture of God.\" And the teacher said, \"But nobody knows what God looks like.\" And the girl said, \"They will, in a minute.\"(Laughter)When my son was four in England â€” Actually, he was four everywhere, to be honest.(Laughter)If we're being strict about it, wherever he went, he was four that year. He was in the Nativity play. Do you remember the story?(Laughter)No, it was big, it was a big story. Mel Gibson did the sequel, you may have seen it.(Laughter)\"Nativity II.\" But James got the part of Joseph, which we were thrilled about. We considered this to be one of the lead parts. We had the place crammed full of agents in T-shirts: \"James Robinson IS Joseph!\" (Laughter) He didn't have to speak, but you know the bit where the three kings come in? They come in bearing gifts, gold, frankincense and myrrh. This really happened. We were sitting there and I think they just went out of sequence, because we talked to the little boy afterward and we said, \"You OK with that?\" And he said, \"Yeah, why? Was that wrong?\" They just switched. The three boys came in, four-year-olds with tea towels on their heads, and they put these boxes down, and the first boy said, \"I bring you gold.\" And the second boy said, \"I bring you myrrh.\" And the third boy said, \"Frank sent this.\"(Laughter)What these things have in common is that kids will take a chance. If they don't know, they'll have a go. Am I right? They're not frightened of being wrong. I don't mean to say that being wrong is the same thing as being creative. What we do know is, if you're not prepared to be wrong, you'll never come up with anything original â€” if you're not prepared to be wrong. And by the time they get to be adults, most kids have lost that capacity. They have become frightened of being wrong. And we run our companies like this. We stigmatize mistakes. And we're now running national education systems where mistakes are the worst thing you can make. And the result is that we are educating people out of their creative capacities.Picasso once said this, he said that all children are born artists. The problem is to remain an artist as we grow up. I believe this passionately, that we don't grow into creativity, we grow out of it. Or rather, we get educated out of it. So why is this?I lived in Stratford-on-Avon until about five years ago. In fact, we moved from Stratford to Los Angeles. So you can imagine what a seamless transition that was.(Laughter)Actually, we lived in a place called Snitterfield, just outside Stratford, which is where Shakespeare's father was born. Are you struck by a new thought? I was. You don't think of Shakespeare having a father, do you? Do you? Because you don't think of Shakespeare being a child, do you? Shakespeare being seven? I never thought of it. I mean, he was seven at some point. He was in somebody's English class, wasn't he?(Laughter)How annoying would that be?(Laughter)\"Must try harder.\"(Laughter)Being sent to bed by his dad, you know, to Shakespeare, \"Go to bed, now! And put the pencil down.\"(Laughter)\"And stop speaking like that.\"(Laughter)\"It's confusing everybody.\"(Laughter)Anyway, we moved from Stratford to Los Angeles, and I just want to say a word about the transition. My son didn't want to come. I've got two kids; he's 21 now, my daughter's 16. He didn't want to come to Los Angeles. He loved it, but he had a girlfriend in England. This was the love of his life, Sarah. He'd known her for a month.(Laughter)Mind you, they'd had their fourth anniversary, because it's a long time when you're 16. He was really upset on the plane, he said, \"I'll never find another girl like Sarah.\" And we were rather pleased about that, frankly â€”(Laughter)Because she was the main reason we were leaving the country.(Laughter)But something strikes you when you move to America and travel around the world: Every education system on Earth has the same hierarchy of subjects. Every one. Doesn't matter where you go. You'd think it would be otherwise, but it isn't. At the top are mathematics and languages, then the humanities, and at the bottom are the arts. Everywhere on Earth. And in pretty much every system too, there's a hierarchy within the arts. Art and music are normally given a higher status in schools than drama and dance. There isn't an education system on the planet that teaches dance everyday to children the way we teach them mathematics. Why? Why not? I think this is rather important. I think math is very important, but so is dance. Children dance all the time if they're allowed to, we all do. We all have bodies, don't we? Did I miss a meeting?(Laughter)Truthfully, what happens is, as children grow up, we start to educate them progressively from the waist up. And then we focus on their heads. And slightly to one side.If you were to visit education, as an alien, and say \"What's it for, public education?\" I think you'd have to conclude, if you look at the output, who really succeeds by this, who does everything that they should, who gets all the brownie points, who are the winners â€” I think you'd have to conclude the whole purpose of public education throughout the world is to produce university professors. Isn't it? They're the people who come out the top. And I used to be one, so there.(Laughter)And I like university professors, but you know, we shouldn't hold them up as the high-water mark of all human achievement. They're just a form of life, another form of life. But they're rather curious, and I say this out of affection for them. There's something curious about professors in my experience â€” not all of them, but typically, they live in their heads. They live up there, and slightly to one side. They're disembodied, you know, in a kind of literal way. They look upon their body as a form of transport for their heads.(Laughter)Don't they? It's a way of getting their head to meetings.(Laughter)If you want real evidence of out-of-body experiences, get yourself along to a residential conference of senior academics, and pop into the discotheque on the final night.(Laughter)And there, you will see it. Grown men and women writhing uncontrollably, off the beat.(Laughter)Waiting until it ends so they can go home and write a paper about it.(Laughter)Our education system is predicated on the idea of academic ability. And there's a reason. Around the world, there were no public systems of education, really, before the 19th century. They all came into being to meet the needs of industrialism. So the hierarchy is rooted on two ideas.Number one, that the most useful subjects for work are at the top. So you were probably steered benignly away from things at school when you were a kid, things you liked, on the grounds that you would never get a job doing that. Is that right? Don't do music, you're not going to be a musician; don't do art, you won't be an artist. Benign advice â€” now, profoundly mistaken. The whole world is engulfed in a revolution.And the second is academic ability, which has really come to dominate our view of intelligence, because the universities designed the system in their image. If you think of it, the whole system of public education around the world is a protracted process of university entrance. And the consequence is that many highly-talented, brilliant, creative people think they're not, because the thing they were good at at school wasn't valued, or was actually stigmatized. And I think we can't afford to go on that way.In the next 30 years, according to UNESCO, more people worldwide will be graduating through education than since the beginning of history. More people, and it's the combination of all the things we've talked about â€” technology and its transformation effect on work, and demography and the huge explosion in population.Suddenly, degrees aren't worth anything. Isn't that true? When I was a student, if you had a degree, you had a job. If you didn't have a job, it's because you didn't want one. And I didn't want one, frankly. (Laughter) But now kids with degrees are often heading home to carry on playing video games, because you need an MA where the previous job required a BA, and now you need a PhD for the other. It's a process of academic inflation. And it indicates the whole structure of education is shifting beneath our feet. We need to radically rethink our view of intelligence.We know three things about intelligence. One, it's diverse. We think about the world in all the ways that we experience it. We think visually, we think in sound, we think kinesthetically. We think in abstract terms, we think in movement. Secondly, intelligence is dynamic. If you look at the interactions of a human brain, as we heard yesterday from a number of presentations, intelligence is wonderfully interactive. The brain isn't divided into compartments. In fact, creativity â€” which I define as the process of having original ideas that have value â€” more often than not comes about through the interaction of different disciplinary ways of seeing things.By the way, there's a shaft of nerves that joins the two halves of the brain called the corpus callosum. It's thicker in women. Following off from Helen yesterday, this is probably why women are better at multi-tasking. Because you are, aren't you? There's a raft of research, but I know it from my personal life. If my wife is cooking a meal at home â€” which is not often, thankfully.(Laughter)No, she's good at some things, but if she's cooking, she's dealing with people on the phone, she's talking to the kids, she's painting the ceiling, she's doing open-heart surgery over here. If I'm cooking, the door is shut, the kids are out, the phone's on the hook, if she comes in I get annoyed. I say, \"Terry, please, I'm trying to fry an egg in here.\"(Laughter)\"Give me a break.\"(Laughter)Actually, do you know that old philosophical thing, if a tree falls in a forest and nobody hears it, did it happen? Remember that old chestnut? I saw a great t-shirt recently, which said, \"If a man speaks his mind in a forest, and no woman hears him, is he still wrong?\"(Laughter)And the third thing about intelligence is, it's distinct. I'm doing a new book at the moment called \"Epiphany,\" which is based on a series of interviews with people about how they discovered their talent. I'm fascinated by how people got to be there. It's really prompted by a conversation I had with a wonderful woman who maybe most people have never heard of, Gillian Lynne. Have you heard of her? Some have. She's a choreographer, and everybody knows her work. She did \"Cats\" and \"Phantom of the Opera.\" She's wonderful. I used to be on the board of The Royal Ballet, as you can see. Anyway, Gillian and I had lunch one day and I said, \"How did you get to be a dancer?\" It was interesting. When she was at school, she was really hopeless. And the school, in the '30s, wrote to her parents and said, \"We think Gillian has a learning disorder.\" She couldn't concentrate; she was fidgeting. I think now they'd say she had ADHD. Wouldn't you? But this was the 1930s, and ADHD hadn't been invented at this point. It wasn't an available condition.(Laughter)People weren't aware they could have that.(Laughter)Anyway, she went to see this specialist. So, this oak-paneled room, and she was there with her mother, and she was led and sat on this chair at the end, and she sat on her hands for 20 minutes while this man talked to her mother about the problems Gillian was having at school. Because she was disturbing people; her homework was always late; and so on, little kid of eight. In the end, the doctor went and sat next to Gillian, and said, \"I've listened to all these things your mother's told me, I need to speak to her privately. Wait here. We'll be back; we won't be very long,\" and they went and left her.But as they went out of the room, he turned on the radio that was sitting on his desk. And when they got out, he said to her mother, \"Just stand and watch her.\" And the minute they left the room, she was on her feet, moving to the music. And they watched for a few minutes and he turned to her mother and said, \"Mrs. Lynne, Gillian isn't sick; she's a dancer. Take her to a dance school.\"I said, \"What happened?\" She said, \"She did. I can't tell you how wonderful it was. We walked in this room and it was full of people like me. People who couldn't sit still. People who had to move to think.\" Who had to move to think. They did ballet, they did tap, jazz; they did modern; they did contemporary. She was eventually auditioned for the Royal Ballet School; she became a soloist; she had a wonderful career at the Royal Ballet. She eventually graduated from the Royal Ballet School, founded the Gillian Lynne Dance Company, met Andrew Lloyd Webber. She's been responsible for some of the most successful musical theater productions in history, she's given pleasure to millions, and she's a multi-millionaire. Somebody else might have put her on medication and told her to calm down.(Applause)What I think it comes to is this: Al Gore spoke the other night about ecology and the revolution that was triggered by Rachel Carson. I believe our only hope for the future is to adopt a new conception of human ecology, one in which we start to reconstitute our conception of the richness of human capacity. Our education system has mined our minds in the way that we strip-mine the earth: for a particular commodity. And for the future, it won't serve us. We have to rethink the fundamental principles on which we're educating our children.There was a wonderful quote by Jonas Salk, who said, \"If all the insects were to disappear from the Earth, within 50 years all life on Earth would end. If all human beings disappeared from the Earth, within 50 years all forms of life would flourish.\" And he's right.What TED celebrates is the gift of the human imagination. We have to be careful now that we use this gift wisely and that we avert some of the scenarios that we've talked about. And the only way we'll do it is by seeing our creative capacities for the richness they are and seeing our children for the hope that they are. And our task is to educate their whole being, so they can face this future. By the way â€” we may not see this future, but they will. And our job is to help them make something of it.Thank you very much.(Applause)\"\"\"\n",
    "result = model(ted, num_sentences=3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Metrics\n",
    "* Convert the audio file into a spectrogram for analysis. \n",
    "* Need the function to split the original .wav file into sentence-wise components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekin\\anaconda3\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import pydub\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "from librosa import feature\n",
    "\n",
    "import soundfile as sf\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function reads in a .wav file. \n",
    "\n",
    "Set cloud to true if data file path is from the cloud.\n",
    "'''\n",
    "def read_wav(file_name, cloud=True):\n",
    "    # read a blob\n",
    "    \n",
    "    if cloud: \n",
    "        blob = bucket.blob(file_name)\n",
    "        file_as_string = blob.download_as_string()\n",
    "\n",
    "        # convert the string to bytes and then finally to audio samples as floats \n",
    "        # and the audio sample rate\n",
    "        data, sample_rate = sf.read(io.BytesIO(file_as_string))\n",
    "    else: \n",
    "        data, sample_rate = librosa.load(file_name)\n",
    "\n",
    "    # print(len(data.shape))\n",
    "    if len(data.shape) == 1: \n",
    "        left_channel = data[:]  # I assume the left channel is column zero\n",
    "    elif len(data.shape) == 2: \n",
    "        left_channel = data[:, 0]  # I assume the left channel is column zero\n",
    "\n",
    "    # enable play button in datalab notebook\n",
    "    aud = ipd.Audio(left_channel, rate=sample_rate)\n",
    "    return aud, data, sample_rate\n",
    "\n",
    "'''\n",
    "Get spectrogram for each audio file. \n",
    "'''\n",
    "def get_spectrogram(y, sr):\n",
    "    spectrogram = librosa.feature.melspectrogram(y, sr)\n",
    "    return spectrogram\n",
    "\n",
    "\n",
    "'''\n",
    "Function to plot a spectrogram \n",
    "'''\n",
    "def plot_spectrogram(spectrogram, y, sr, title='Mel-frequency spectrogram'):\n",
    "    fig, ax = plt.subplots()\n",
    "    S_dB = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "    img = librosa.display.specshow(S_dB, x_axis='time',\n",
    "                             y_axis='mel', sr=sr,\n",
    "                             fmax=8000, ax=ax)\n",
    "    fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "    ax.set(title=title)\n",
    "    plt.show()\n",
    "    \n",
    "'''\n",
    "This function splits the spectrogram into chunks of size \n",
    "length. \n",
    "'''\n",
    "def center_crop(spec, length):\n",
    "    spec_size = spec.shape[-1]\n",
    "    \n",
    "    mid = spec_size // 2\n",
    "    \n",
    "    length_mid = length // 2\n",
    "    start = mid - length_mid \n",
    "    end = mid + length_mid\n",
    "    return spec[:, start:end]\n",
    "\n",
    "'''\n",
    "Convert file to spectrogram\n",
    "'''\n",
    "def create_datasets(directory, verbose=0):\n",
    "    X = []\n",
    "\n",
    "    # Get feature vector from .wav\n",
    "    data, sample_rate = librosa.load(file_path)\n",
    "#             feature_vector = get_feature_vector(data, sample_rate)\n",
    "    spectrogram = get_spectrogram(data, sample_rate)\n",
    "    spectrogram = center_crop(spectrogram, 124)\n",
    "    \n",
    "    X.append(spectrogram) # add to dset list\n",
    "\n",
    "    X = np.array(X)\n",
    "    return X, y, y_bin     \n",
    "\n",
    "def perform_audio_analysis(directory):\n",
    "    return directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio analysis (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised\n",
    "def get_audio_analysis(audio_analysis):\n",
    "    audio_categories = []\n",
    "    total = 0\n",
    "    sums = {'Neutral': 0, 'Happy': 0, 'Sad': 0, 'Very Expressive': 0}\n",
    "    for analysis in audio_analysis: \n",
    "        index = np.argmax(analysis)\n",
    "        if index == 0 or index == 1:\n",
    "            audio_categories.append('Neutral')\n",
    "            sums['Neutral'] += 1\n",
    "        elif index == 2:\n",
    "            audio_categories.append('Happy')\n",
    "            sums['Happy'] += 1\n",
    "        elif index == 3: \n",
    "            audio_categories.append('Sad')\n",
    "            sums['Sad'] += 1\n",
    "        else: \n",
    "            audio_categories.append('Very Expressive')\n",
    "            sums['Very Expressive'] += 1\n",
    "        total += 1\n",
    "\n",
    "    percentages = {\n",
    "        'Neutral': sums['Neutral'] / total, \n",
    "        'Happy': sums['Happy'] / total, \n",
    "        'Sad': sums['Sad'] / total,\n",
    "        'Very Expressive': sums['Very Expressive'] / total,\n",
    "    }\n",
    "    return audio_categories, percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intensity analysis (0 = neutral, 1 = strong/passionate)\n",
    "def get_intensity_analysis(emotion_intensity):\n",
    "    emotions = []\n",
    "    total = 0\n",
    "    sums = {0: 0, 1: 0}\n",
    "    for emotion in emotion_intensity: \n",
    "        if emotion[0] >= 0.5:\n",
    "            emotions.append('Passionate')\n",
    "            sums[1] += 1\n",
    "        else:         \n",
    "            emotions.append('Neutral')\n",
    "            sums[0] += 1\n",
    "        total += 1\n",
    "    percentages = {'Neutral': sums[0] / total, 'Passionate': sums[1] / total}\n",
    "    return emotions, percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Full audio analysis function given bucket name \n",
    "and file name. \n",
    "\n",
    "Model paths are kept as optional params. \n",
    "'''\n",
    "\n",
    "def perform_audio_analysis(bucket_name, filename, \n",
    "                          audio_model_filepath = './audio_analysis_model', emotion_intensity_model_filepath = './emotion_intensity_model'):\n",
    "    import pydub\n",
    "\n",
    "    import IPython.display as ipd\n",
    "\n",
    "    import librosa\n",
    "    import librosa.display\n",
    "    from librosa import feature\n",
    "\n",
    "    import soundfile as sf\n",
    "    import io\n",
    "    import os\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/ekin/Downloads/hack.json\"\n",
    "    \n",
    "    # Create a Cloud Storage client.\n",
    "    gcs = storage.Client()\n",
    "\n",
    "    # Get the bucket that the file will be uploaded to.\n",
    "    bucket = gcs.get_bucket(bucket_name)\n",
    "    aud, y, sr = read_wav(example_file_name)\n",
    "    \n",
    "    # Convert into spectrogram\n",
    "    spectrogram = get_spectrogram(y[:, 0], sr)\n",
    "    \n",
    "    # Load models\n",
    "    audio_model = tf.keras.models.load_model(audio_model_filepath)\n",
    "    emotion_intensity_model = tf.keras.models.load_model(emotion_intensity_model_filepath)\n",
    "    \n",
    "    # Break into time-based chunks, every ~3 seconds \n",
    "    num_chunks = spectrogram.shape[-1] // 124\n",
    "    sound_chunks = []\n",
    "    for i in range(num_chunks): \n",
    "        chunk = spectrogram[:, i * 124 : (i+1) * 124]\n",
    "        sound_chunks.append(chunk)\n",
    "    sound_chunks = np.array(sound_chunks)\n",
    "    sound_chunks = np.expand_dims(sound_chunks, axis=-1)\n",
    "    \n",
    "    # Make predictions\n",
    "    audio_analysis = audio_model.predict(sound_chunks)\n",
    "    emotion_intensity = emotion_intensity_model.predict(sound_chunks)\n",
    "\n",
    "    audio_categories, audio_percentages = get_audio_analysis(audio_analysis)\n",
    "    emotions, emotion_percentages = get_intensity_analysis(emotion_intensity)\n",
    "    \n",
    "    # Classifies audio into more descriptive emotions\n",
    "    audio_stats = (audio_categories, audio_percentages)\n",
    "    \n",
    "    # Classifies audio based on neutrality vs. strength (aka passion/intensity)\n",
    "    emote_stats = (emotions, emotion_percentages)\n",
    "    return audio_stats, emote_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example Usage\n",
    "'''\n",
    "\n",
    "bucket_name = 'hack_the_ne'\n",
    "filename = 'example_1.wav'\n",
    "audio_model_filepath = './audio_analysis_model'\n",
    "emotion_intensity_model_filepath = './emotion_intensity_model'\n",
    "\n",
    "audio_stats, emote_stats = perform_audio_analysis(bucket_name, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Very Expressive', 'Sad', 'Neutral', 'Very Expressive', 'Sad', 'Sad', 'Neutral', 'Sad', 'Neutral', 'Neutral', 'Sad', 'Very Expressive', 'Very Expressive', 'Sad', 'Very Expressive', 'Neutral', 'Neutral', 'Neutral'], {'Neutral': 0.3888888888888889, 'Happy': 0.0, 'Sad': 0.3333333333333333, 'Very Expressive': 0.2777777777777778})\n",
      "\n",
      "\n",
      "(['Neutral', 'Neutral', 'Neutral', 'Passionate', 'Neutral', 'Passionate', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Passionate', 'Neutral', 'Passionate', 'Neutral', 'Neutral', 'Neutral', 'Neutral'], {'Neutral': 0.7777777777777778, 'Passionate': 0.2222222222222222})\n"
     ]
    }
   ],
   "source": [
    "print(audio_stats)\n",
    "print('\\n')\n",
    "print(emote_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaze-tracking Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "This function reads in a .wav file. \n",
    "\n",
    "Set cloud to true if data file path is from the cloud.\n",
    "'''\n",
    "\n",
    "def generate_image_url(file_path, bucket):\n",
    "    \"\"\" generate signed URL of a video stored on google storage. \n",
    "        Valid for 300 seconds in this case. You can increase this \n",
    "        time as per your requirement. \n",
    "    \"\"\"                                                        \n",
    "    blob = bucket.blob(file_path) \n",
    "    return blob.generate_signed_url(datetime.timedelta(seconds=300), method='GET')\n",
    "\n",
    "def setup(filename, bucket_name):\n",
    "    # Create a Cloud Storage client.\n",
    "    gcs = storage.Client()\n",
    "\n",
    "    # Get the bucket that the file will be uploaded to.\n",
    "    \n",
    "    bucket = gcs.get_bucket(bucket_name)\n",
    "    \n",
    "    url = generate_image_url(filename, bucket)\n",
    "    req.urlretrieve(url, filename)\n",
    "    cap = cv2.VideoCapture(filename)\n",
    "    return cap\n",
    "\n",
    "def track_gaze(frame, blinking):\n",
    "    gaze = GazeTracking()\n",
    "    gaze.refresh(frame)\n",
    "\n",
    "    frame = gaze.annotated_frame()\n",
    "    text = \"\"\n",
    "\n",
    "#     print('Horizontal Ratio: ', gaze.horizontal_ratio())\n",
    "#     print('Vertical Ratio: ', gaze.vertical_ratio())\n",
    "    \n",
    "    # can I add a look up and look down? \n",
    "    if gaze.is_blinking():\n",
    "        text = \"center\"\n",
    "    elif gaze.is_right():\n",
    "        # Right\n",
    "        text = \"right\"\n",
    "    elif gaze.is_left():\n",
    "        # Left\n",
    "        text = \"left\"\n",
    "    elif gaze.is_center() and gaze.is_down():\n",
    "        # Down\n",
    "        text = \"down\"\n",
    "    elif gaze.is_center() and gaze.is_up():\n",
    "        # Up\n",
    "        text = \"up\"\n",
    "    elif gaze.is_center():\n",
    "        # Center\n",
    "        text = \"center\"\n",
    "    else: \n",
    "        text = \"other\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "def track_eyes(cap, ms_per_frame=1000, blinking=True):\n",
    "    images_eyes = {} # timestamp (sec, int) : eyes position classification (str)\n",
    "    frames = []\n",
    "    if cap.isOpened():\n",
    "        print (\"File Can be Opened\")\n",
    "\n",
    "        success, image = cap.read()\n",
    "        count = 0\n",
    "        while success:\n",
    "    #         cv2.imwrite(\"frame%d.jpg\" % count, image)     # save frame as JPEG file \n",
    "            cap.set(cv2.CAP_PROP_POS_MSEC,(count*ms_per_frame))    # added this line \n",
    "            success, image = cap.read()\n",
    "            \n",
    "            if image is None: \n",
    "                break\n",
    "            frames.append(image)\n",
    "            gaze = track_gaze(image, blinking)\n",
    "            images_eyes[count] = gaze\n",
    "#             print('Read a new frame: ', success)\n",
    "            count += 1\n",
    "#             print(count)\n",
    "        \n",
    "        # When everything done, release the capture\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print (\"Video stop\")\n",
    "    else:\n",
    "        print(\"Not Working\")\n",
    "    \n",
    "    return frames, images_eyes\n",
    "\n",
    "'''\n",
    "This function returns `frames`, the images themselves, and the gaze classification \n",
    "for each timestamp (every second).\n",
    "'''\n",
    "def perform_gaze_tracking(filename, bucket_name):\n",
    "    import sys\n",
    "    from google.cloud import storage\n",
    "\n",
    "    from tqdm.notebook import tqdm\n",
    "    import datetime\n",
    "    import os\n",
    "    import cv2\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/ekin/Downloads/hack.json\"\n",
    "\n",
    "    import urllib.request as req\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.image as mpimg\n",
    "\n",
    "    from GazeTracking.gaze_tracking import GazeTracking\n",
    "    \n",
    "    cap = setup(filename, bucket_name)\n",
    "    frames, images_eyes = track_eyes(cap)\n",
    "    return frames, images_eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Can be Opened\n",
      "Video stop\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Example Usage\n",
    "'''\n",
    "filename = 'sample1.mp4'\n",
    "bucket_name = 'hack_the_ne'\n",
    "\n",
    "frames, images_eyes = perform_gaze_tracking(filename, bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'blinking', 1: 'blinking', 2: 'blinking', 3: 'blinking', 4: 'blinking', 5: 'blinking', 6: 'blinking', 7: 'blinking', 8: 'blinking', 9: 'blinking', 10: 'blinking', 11: 'blinking', 12: 'blinking', 13: 'blinking', 14: 'blinking', 15: 'blinking', 16: 'blinking', 17: 'blinking', 18: 'blinking', 19: 'other', 20: 'blinking', 21: 'blinking', 22: 'blinking', 23: 'blinking', 24: 'blinking', 25: 'blinking', 26: 'blinking'}\n"
     ]
    }
   ],
   "source": [
    "print(images_eyes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frames):\n",
    "    for idx, image in enumerate(frames): \n",
    "        classification = images_eyes[idx]\n",
    "\n",
    "        plt.imshow(image)\n",
    "        plt.title(classification)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial Emotion Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(image):\n",
    "    dim = (48, 48, 1)\n",
    "    resized = cv2.resize(image, dim)\n",
    "    gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "    return gray\n",
    "\n",
    "def partitition_video():\n",
    "    model = tf.keras.models.load_model('./facial_exp_2_cl')\n",
    "    \n",
    "    frames = []\n",
    "    emotions = []\n",
    "    if cap.isOpened():\n",
    "        print (\"File Can be Opened\")\n",
    "\n",
    "        success, image = cap.read()\n",
    "        count = 0\n",
    "        while success:\n",
    "    #         cv2.imwrite(\"frame%d.jpg\" % count, image)     # save frame as JPEG file \n",
    "            cap.set(cv2.CAP_PROP_POS_MSEC,(count*ms_per_frame))    # added this line \n",
    "            success, image = cap.read()\n",
    "            \n",
    "            if image is None: \n",
    "                break\n",
    "            image = process_img(image)\n",
    "        \n",
    "            frames.append(image)\n",
    "            pred = model.predict(image)\n",
    "            emotions.append(pred)\n",
    "    #             print('Read a new frame: ', success)\n",
    "            count += 1\n",
    "    #             print(count)\n",
    "\n",
    "        # When everything done, release the capture\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print (\"Video stop\")\n",
    "    else:\n",
    "        print(\"Not Working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
